{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09db231f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f158ac90",
   "metadata": {},
   "source": [
    "# Run Experiment  One and Read in Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e32f693",
   "metadata": {},
   "source": [
    "* Use one-word term\n",
    "* Each term has a 100% chance or 75% chance to be included in the index or fingerprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14d748d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# POWERS = [0, 4, 3, 2, 1]\n",
    "POWERS = [0, 2]\n",
    "CHANCE = ['100', '93.75', '87.5', '75', '50'] # percentage\n",
    "INDEX_TIME_UNIT = 1000 # number of tweets\n",
    "QUERY_TIME_UNIT = 1000\n",
    "SIMILARITY_THRE = 0.5\n",
    "NUMBER_OF_EPOCHS = 3\n",
    "OUTPUT_DIR = \"./output_expm1/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d895eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function runs the experiment and compares the results including:\n",
    "# - the size of the index and doc lists \n",
    "# - time for constructing the index and executing query \n",
    "# - number of lookups before finding the match\n",
    "\n",
    "def run_expm1():\n",
    "#     for p in POWERS:\n",
    "#         runSketchTweets(\"SketchingTweet-v1\", INDEX_TIME_UNIT, QUERY_TIME_UNIT, p, SIMILARITY_THRE, NUMBER_OF_EPOCHS, OUTPUT_DIR)\n",
    "    summary, termfreq, query_result_similarity_lookup, index_time, query_time = load_expm1_csv(INDEX_TIME_UNIT, QUERY_TIME_UNIT, 1, similarity, n, output_dir)\n",
    "    return summary, termfreq, query_result_similarity_lookup, index_time, query_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfece70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runSketchTweets(jar, index_time_unit, query_time_unit, chance_power, similarity_threshold, n, output_dir):\n",
    "    for epoch in range (1, n+1):\n",
    "        os.system(\"java -jar ./{}.jar -iu {} -qu {} -p {} -s {} -n {} -o {}\"\n",
    "        .format(jar, index_time_unit, query_time_unit, chance_power, similarity_threshold, epoch, output_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa16c4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_expm1_csv(index_time_unit, query_time_unit, power, similarity, n, directory):\n",
    "    '''\n",
    "    Returns a list of results.\n",
    "    The results are ordered in the form of [summary, termfreq, query_results_similarity_lookups, index_time, query_time] \n",
    "    '''\n",
    "\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            filename = re.match( r'Epoch_(\\d+)_iu_(\\d+)_qu_(\\d+)_p_(\\d+)_s_([\\.\\d]+)_(.+).csv', file)  \n",
    "#             print(filename.group(3),filename.group(4),filename.group(5),filename.group(6))\n",
    "            name = filename.group(6)\n",
    "            power = filename.group(4)\n",
    "\n",
    "            if name == \"summary\":\n",
    "                if power == '0':\n",
    "                    summary0 = pd.read_csv(directory + \"/\" + file, header=None)\n",
    "                elif power == '2':\n",
    "                    summary1 = pd.read_csv(directory + \"/\" + file, header=None)\n",
    "\n",
    "            if name == \"term_freq\":\n",
    "                if power == '0':\n",
    "                    termfreq0 = pd.read_csv(directory + \"/\" + file, header=None)\n",
    "                elif power == '2':\n",
    "                    termfreq1 = pd.read_csv(directory + \"/\" + file, header=None)\n",
    "                    \n",
    "            if name == \"query_result\":\n",
    "                if power == '0':\n",
    "                    query_result0 = pd.read_csv(directory + \"/\" + file, header=None)\n",
    "                elif power == '2':\n",
    "                    query_result1 = pd.read_csv(directory + \"/\" + file, header=None)\n",
    "                    \n",
    "            if name == \"query_similarity\":\n",
    "                if power == '0':\n",
    "                    query_similarity0 = pd.read_csv(directory + \"/\" + file, header=None)\n",
    "                elif power == '2':\n",
    "                    query_similarity1 = pd.read_csv(directory + \"/\" + file, header=None)\n",
    "                    \n",
    "            if name == \"lookups\":\n",
    "                if power == '0':\n",
    "                    lookups0 = pd.read_csv(directory + \"/\" + file, header=None)\n",
    "                elif power == '2':\n",
    "                    lookups1 = pd.read_csv(directory + \"/\" + file, header=None)\n",
    "            \n",
    "            if name == \"index_time\":\n",
    "                epoch = filename.group(1)\n",
    "                if power == '0':\n",
    "                    if epoch == '1':\n",
    "                        index_time0_1 = pd.read_csv(directory + \"/\" + file, header=None)\n",
    "                    elif epoch == '2':\n",
    "                        index_time0_2 = pd.read_csv(directory + \"/\" + file, header=None)\n",
    "                    elif epoch == '3':\n",
    "                        index_time0_3 = pd.read_csv(directory + \"/\" + file, header=None)\n",
    "                elif power == '2':\n",
    "                    if epoch == '1':\n",
    "                        index_time1_1 = pd.read_csv(directory + \"/\" + file, header=None)\n",
    "                    elif epoch == '2':\n",
    "                        index_time1_2 = pd.read_csv(directory + \"/\" + file, header=None)\n",
    "                    elif epoch == '3':\n",
    "                        index_time1_3 = pd.read_csv(directory + \"/\" + file, header=None)\n",
    "                    \n",
    "            if name == \"query_time\":\n",
    "                epoch = filename.group(1)\n",
    "                if power == '0':\n",
    "                    if epoch == '1':\n",
    "                        query_time0_1 = pd.read_csv(directory + \"/\" + file, header=None)\n",
    "                    elif epoch == '2':\n",
    "                        query_time0_2 = pd.read_csv(directory + \"/\" + file, header=None)\n",
    "                    elif epoch == '3':\n",
    "                        query_time0_3 = pd.read_csv(directory + \"/\" + file, header=None)\n",
    "                elif power == '2':\n",
    "                    if epoch == '1':\n",
    "                        query_time1_1 = pd.read_csv(directory + \"/\" + file, header=None)\n",
    "                    elif epoch == '2':\n",
    "                        query_time1_2 = pd.read_csv(directory + \"/\" + file, header=None)\n",
    "                    elif epoch == '3':\n",
    "                        query_time1_3 = pd.read_csv(directory + \"/\" + file, header=None)\n",
    "                        \n",
    "\n",
    "    summary = pd.concat([summary0, summary1.iloc[:,1]], 1)\n",
    "    summary.columns = ['', '100% chance','75% chance']\n",
    "#     print(summary)\n",
    "    \n",
    "    termfreq0.columns = ['term', '100% chance']\n",
    "    termfreq1.columns = ['term','75% chance']\n",
    "    termfreq = (termfreq0, termfreq1)\n",
    "#     print(termfreq[0])\n",
    "#     print(termfreq[1])\n",
    "    \n",
    "    query_result_similarity_lookup = pd.concat([query_result0, query_result1, query_similarity0, query_similarity1, lookups0, lookups1], 1)\n",
    "    query_result_similarity_lookup.columns = ['result_100%','result_75%', 'similarity_100%','similarity_75%', 'num_of_lookups_100%','num_of_lookups_75%']\n",
    "#     print(query_result_similarity_lookup)\n",
    "    \n",
    "    index_time = pd.concat([index_time0_1, index_time0_2, index_time0_3, index_time1_1, index_time1_2, index_time1_3], 1)\n",
    "    index_time.columns = ['index_time_100%_epoch1','index_time_100%_epoch2','index_time_100%_epoch3',\n",
    "                          'index_time_75%_epoch1','index_time_75%_epoch2','index_time_75%_epoch3']\n",
    "#     print(index_time)\n",
    "        \n",
    "    query_time = pd.concat([query_time0_1, query_time0_2, query_time0_3, query_time1_1, query_time1_2, query_time1_3], 1)\n",
    "    query_time.columns = ['query_time_100%_epoch1','query_time_100%_epoch2','query_time_100%_epoch3',\n",
    "                          'query_time_75%_epoch1','query_time_75%_epoch2','query_time_75%_epoch3']\n",
    "#     print(query_time)\n",
    "\n",
    "    return summary, termfreq, query_result_similarity_lookup, index_time, query_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "acba0674",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'query_time_unit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-7c975395739f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msummary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtermfreq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquery_result_similarity_lookup\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex_time\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquery_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrun_expm1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-7-6fb20e64bd0e>\u001b[0m in \u001b[0;36mrun_expm1\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mPOWERS\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mrunSketchTweets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"SketchingTweet-v1\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mINDEX_TIME_UNIT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mQUERY_TIME_UNIT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSIMILARITY_THRE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNUMBER_OF_EPOCHS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mOUTPUT_DIR\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0msummary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtermfreq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquery_result_similarity_lookup\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex_time\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquery_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_expm1_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mINDEX_TIME_UNIT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquery_time_unit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msimilarity\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0msummary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtermfreq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquery_result_similarity_lookup\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex_time\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquery_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'query_time_unit' is not defined"
     ]
    }
   ],
   "source": [
    "summary, termfreq, query_result_similarity_lookup, index_time, query_time = run_expm1()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6280ff18",
   "metadata": {},
   "source": [
    "# Process Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1197f85f",
   "metadata": {},
   "source": [
    "## Index size comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b864b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a1f5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_index_size(sizes, title):\n",
    "    data = pd.to_numeric(sizes) \n",
    "    ratio = round(((data['100% chance'] - data['75% chance']) / data['100% chance']) * 100, 3)\n",
    "    layer = sns.barplot(x=data.index, y=data.to_list(), color=\"#66B3BA\")\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "    print('The index size is ' + str(ratio) + \"% smaller when terms has a 75% chance to be indexed or included in the fingerfrints.\")\n",
    "\n",
    "\n",
    "compare_index_size(summary.iloc[7,1:], \"The index sizes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff24d24",
   "metadata": {},
   "source": [
    "## Documnet list length comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d9b110",
   "metadata": {},
   "outputs": [],
   "source": [
    "termfreq[0][\"100% chance\"].value_counts() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28169455",
   "metadata": {},
   "outputs": [],
   "source": [
    "termfreq[1][\"75% chance\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b45c9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_doclist_len(termfreq):\n",
    "    lengths = [np.array(i.iloc[:,1:2].squeeze()) for i in termfreq]\n",
    "    avgs = [np.average(l) for l in lengths]\n",
    "    medians = [np.median(l) for l in lengths]\n",
    "    print('The average document list length: ')\n",
    "    print(avgs)\n",
    "    layer1 = sns.histplot(data=termfreq[0], log_scale=True, color=\"#66B3BA\")\n",
    "    plt.show()\n",
    "    layer2 = sns.histplot(data=termfreq[1], log_scale=True, color=\"#77B3BA\")\n",
    "    plt.show()\n",
    "\n",
    "compare_doclist_len(termfreq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a9331f",
   "metadata": {},
   "source": [
    "## Query Recall\n",
    "what we searched correctly / what are truely similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3ff66b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "query_result_similarity_lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c62864",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_true_similar = query_result_similarity_lookup['result_100%'] != -1\n",
    "true_similar = query_result_similarity_lookup[is_true_similar]\n",
    "num_true_similar = true_similar.shape[0]\n",
    "true_similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6d173f",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_correct_search = true_similar['result_100%'] == true_similar['result_75%']\n",
    "correct_search = true_similar[is_correct_search]\n",
    "num_correct_search = correct_search.shape[0]\n",
    "correct_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed372fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "recall = round((num_correct_search/num_true_similar) * 100, 2)\n",
    "print('Using 75% terms, we can find {}% true similar tweets'.format(recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af51d11f",
   "metadata": {},
   "source": [
    "## Query Precision\n",
    "what we searched correctly / what are think are similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903fba63",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_predict_similar = query_result_similarity_lookup['result_75%'] != -1\n",
    "predict_similar = query_result_similarity_lookup[is_predict_similar]\n",
    "num_predict_similar = predict_similar.shape[0]\n",
    "predict_similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79b3e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = round((num_correct_search/num_predict_similar) * 100, 2)\n",
    "print('Using 75% terms, {}% tweets that we found similar are true similar tweets'.format(precision))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918f6b80",
   "metadata": {},
   "source": [
    "## Number of lookups in correct searches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b70089",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_less_search = correct_search['num_of_lookups_100%'] > correct_search['num_of_lookups_75%']\n",
    "less_search = correct_search[is_less_search]\n",
    "num_less_search = less_search.shape[0]\n",
    "less_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0185ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "proportion = round((num_less_search / num_correct_search) * 100, 2)\n",
    "print('Using 75% terms, {}% tweets need less lookups before correctly finding the true similar tweets'.format(proportion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e192340",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_lookups_100 = np.average(np.array(correct_search['num_of_lookups_100%']))\n",
    "avg_lookups_75 = np.average(np.array(correct_search['num_of_lookups_75%']))\n",
    "reduce = round(((avg_lookups_100 - avg_lookups_75) / avg_lookups_100) * 100, 2)\n",
    "print('Using 75% terms, {}% less lookups are needed for correctly finding the true similar tweets'.format(reduce))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f40c5a",
   "metadata": {},
   "source": [
    "## Index time comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0e4779",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "index_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6815d6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_time_avgs = []\n",
    "for col in index_time.columns:\n",
    "    index_time_avgs.append(np.average(np.array(index_time[col])))\n",
    "\n",
    "index_time_avgs2 = []\n",
    "c = 0\n",
    "acc = 0\n",
    "for avg in index_time_avgs:\n",
    "    if c < 2:\n",
    "        acc += avg\n",
    "        c += 1\n",
    "        \n",
    "    else:\n",
    "        acc += avg\n",
    "#         print(acc)\n",
    "        index_time_avgs2.append(round(acc/3 , 4))\n",
    "        acc = 0\n",
    "        c = 0\n",
    "\n",
    "index_time_avgs2 \n",
    "print('Using 100% terms, the average time for indexing 1000 tweets is {} milliseconds'.format(index_time_avgs2[0]))\n",
    "print('Using 75% terms, the average time for indexing 1000 tweets is {} milliseconds'.format(index_time_avgs2[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff571abd",
   "metadata": {},
   "source": [
    "## Query time comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862e2dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1d059e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_time_avgs = []\n",
    "for col in query_time.columns:\n",
    "    query_time_avgs.append(np.average(np.array(query_time[col])))\n",
    "\n",
    "query_time_avgs2 = []\n",
    "c = 0\n",
    "acc = 0\n",
    "for avg in query_time_avgs:\n",
    "    if c < 2:\n",
    "        acc += avg\n",
    "        c += 1\n",
    "        \n",
    "    else:\n",
    "        acc += avg\n",
    "#         print(acc)\n",
    "        query_time_avgs2.append(round(acc/3 , 4))\n",
    "        acc = 0\n",
    "        c = 0\n",
    "\n",
    "query_time_avgs2 \n",
    "print('Using 100% terms, the average time for querying 1000 tweets is {} milliseconds'.format(query_time_avgs2[0]))\n",
    "print('Using 75% terms, the average time for querying 1000 tweets is {} milliseconds'.format(query_time_avgs2[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd1673c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
